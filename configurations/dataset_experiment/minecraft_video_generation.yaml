# @package _global_
# 12 x H100 GPUs

# When the (dataset, experiment) pair aligns with the file name of this yaml, 
# the values here will override individual yamls files for dataset, algorithm and experiment.
# useful for dataset-specific overrides
defaults:
  # - ../algorithm/backbone@algorithm.backbone: dit3d
  - ../algorithm@algorithm.vae: dc_ae_16x_preprocessor # dc_ae_preprocessor, kl_autoencoder_preprocessor
dataset:
  latent:
    enabled: True
  # subdataset_size: 64000
  num_eval_videos: 768
  max_frames: 16

algorithm:
  lr_scheduler:
    name: constant_with_warmup
    num_warmup_steps: 50000
    num_training_steps: 1000000
  weight_decay: 0.001
  compile: False
  diffusion:
    loss_weighting:
      strategy: sigmoid
      sigmoid_bias: -1.0
    training_schedule:
      name: cosine
      shift: 0.125
    beta_schedule: cosine_simple_diffusion
    schedule_fn_kwargs:
      shifted: 0.125
  uniform_future:
    enabled: true
  backbone:
    patch_size: 2
    external_cond_dropout: 0.1
  vae:
    # DFoT vae: pretrained:ImageVAE_MCRAFT.ckpt
    # FAR vae: /scratch/s224075134/temporal_diffusion/FAR/pretrained/dcae/DCAE_Minecraft_Res128-a5677f66.pth
    pretrained_path: /scratch/s224075134/temporal_diffusion/FAR/pretrained/dcae/DCAE_Minecraft_Res128-a5677f66.pth 
    batch_size: 2
  logging:
    max_num_videos: 32

  contrastive_clip_frames: 8
  diffusion_clip_frames: 50
  contrastive_loss_weight: 0.3

experiment:
  training:
    lr: 1e-4
    batch_size: 8
    max_epochs: 10
    data:
      num_workers: 11
      shuffle: False
    checkpointing:
      every_n_epochs: 1
      every_n_train_steps: null
      save_last: True
      save_top_k: -1
  validation:
    batch_size: 16
    val_every_n_step: 25000
    val_every_n_epoch: null
    validate_training_set: True
    validate_history_free: True
    limit_batch: 50
    data:
      num_workers: 1
  test:
    batch_size: 16
    data:
      num_workers: 1
